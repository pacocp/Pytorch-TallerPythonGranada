{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T12:26:25.898953Z",
     "start_time": "2019-04-02T12:26:25.895262Z"
    }
   },
   "source": [
    "## Author: Francisco Carrillo-Perez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generative Adversarial Networks\n",
    "\n",
    "In this notebook we are going to:\n",
    "   - Learn to code our first Generative Adversarial Network.\n",
    "   - Learn to train and test our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Utils functions\n",
    "\n",
    "First we are going to code some useful functions. One of them will be the conversion from images to vectors and from vectos to images, in order to represent mnist images for generator and discriminator. Also, we will do a function to return n samples of Gaussian noise, for the generator input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T12:43:07.453546Z",
     "start_time": "2019-04-02T12:43:07.436785Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def mnist_dataset():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "    out_dir = \"./dataset\"\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "def images_to_vectors(images):\n",
    "    \"\"\"\n",
    "    Flatten 28x28 image to 784 vector\n",
    "    \"\"\"\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    \"\"\"\n",
    "    784 vector to 28x28 images\n",
    "    \"\"\"\n",
    "    return vectors.view(vectors.size(0),1, 28, 28)\n",
    "\n",
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    return n\n",
    "\n",
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    return data\n",
    "\n",
    "def save_figures(images,path=\"./generated/\"):\n",
    "    # if dir doesnot exist, create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    name = \"generated.png\"\n",
    "    save_image(images,path+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generator\n",
    "\n",
    "Generator is a model that aims to generate new data similar to the expected one. G(z, θ₁) is used to model the Generator. It’s role is mapping input noise variables z to the desired data space x (say images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T12:40:51.299807Z",
     "start_time": "2019-04-02T12:40:51.285536Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Three hidden layer generative neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100 # latent variable vector\n",
    "        n_out = 784 # image generated\n",
    "\n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.00002)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, discriminator, fake_data):\n",
    "        loss = nn.BCELoss()\n",
    "        N = fake_data.size(0)\n",
    "\n",
    "        # Reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Sample noise and generate fake data\n",
    "        prediction = discriminator(fake_data)\n",
    "\n",
    "        # Calculate error and backpropagate\n",
    "        error = loss(prediction, ones_target(N))\n",
    "        error.backward()\n",
    "\n",
    "        # Update weights with gradient\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Discriminator\n",
    "\n",
    "The Discriminator model, however, aims to recognize if an input data is ‘real’ — belongs to the original dataset — or if it is ‘fake’ — generated by a forger. D(x, θ₂) models the discriminator and outputs the probability that the data came from the real dataset, in the range (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T12:40:55.237974Z",
     "start_time": "2019-04-02T12:40:55.214745Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Three hidden layer discriminative NN\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 784 # 28x28\n",
    "        n_out = 1 # fake or real\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(256, n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.00002)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, real_data, fake_data):\n",
    "        loss = nn.BCELoss()\n",
    "        N = real_data.size(0)\n",
    "        # reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # 1.1 Train on real data\n",
    "        prediction_real = self(real_data)\n",
    "        # Calculate error and backpropagate\n",
    "        error_real = loss(prediction_real, ones_target(N))\n",
    "        error_real.backward()\n",
    "\n",
    "        # 1.2 Train on fake data\n",
    "        prediction_fake = self(fake_data)\n",
    "        # calculate error and backpropagate\n",
    "        error_fake = loss(prediction_fake, zeros_target(N))\n",
    "        error_fake.backward()\n",
    "\n",
    "        # 1.3 Update weights with gradients\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Return error and predictions for real and fake inputs\n",
    "        return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train our model\n",
    "Let's train our two architectures at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:02:01.535152Z",
     "start_time": "2019-04-02T12:46:08.575361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "-------\n",
      "Discriminator error: 1.360549807548523\n",
      "\n",
      "\n",
      "Generator error: 0.6761853098869324\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.035473108291626\n",
      "\n",
      "\n",
      "Generator error: 0.5511122941970825\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.0761303901672363\n",
      "\n",
      "\n",
      "Generator error: 0.6966161131858826\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.93159019947052\n",
      "\n",
      "\n",
      "Generator error: 0.9237263202667236\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.694118082523346\n",
      "\n",
      "\n",
      "Generator error: 1.2150769233703613\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.6711529493331909\n",
      "\n",
      "\n",
      "Generator error: 1.195939540863037\n",
      "-------\n",
      "Saving samples\n",
      "-------\n",
      "Discriminator error: 0.5739818215370178\n",
      "\n",
      "\n",
      "Generator error: 1.3262486457824707\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.3923016786575317\n",
      "\n",
      "\n",
      "Generator error: 0.6596801280975342\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.1216189861297607\n",
      "\n",
      "\n",
      "Generator error: 0.855226457118988\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.108191967010498\n",
      "\n",
      "\n",
      "Generator error: 0.9839785695075989\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 1.0717107057571411\n",
      "\n",
      "\n",
      "Generator error: 0.9322816729545593\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.7074973583221436\n",
      "\n",
      "\n",
      "Generator error: 1.3729219436645508\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.5484475493431091\n",
      "\n",
      "\n",
      "Generator error: 1.5898420810699463\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.4354577958583832\n",
      "\n",
      "\n",
      "Generator error: 1.738468050956726\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.34496137499809265\n",
      "\n",
      "\n",
      "Generator error: 1.888918161392212\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.2891557514667511\n",
      "\n",
      "\n",
      "Generator error: 2.010528326034546\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.15322591364383698\n",
      "\n",
      "\n",
      "Generator error: 2.0704026222229004\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.10860079526901245\n",
      "\n",
      "\n",
      "Generator error: 2.922177314758301\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.3362252116203308\n",
      "\n",
      "\n",
      "Generator error: 2.15580415725708\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.12581714987754822\n",
      "\n",
      "\n",
      "Generator error: 3.1742641925811768\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.11596305668354034\n",
      "\n",
      "\n",
      "Generator error: 3.521468162536621\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.08051788806915283\n",
      "\n",
      "\n",
      "Generator error: 3.532191276550293\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.0992509052157402\n",
      "\n",
      "\n",
      "Generator error: 4.184208393096924\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.20678994059562683\n",
      "\n",
      "\n",
      "Generator error: 3.740447998046875\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.22082412242889404\n",
      "\n",
      "\n",
      "Generator error: 3.5644118785858154\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.08442331850528717\n",
      "\n",
      "\n",
      "Generator error: 3.8294951915740967\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.08074364066123962\n",
      "\n",
      "\n",
      "Generator error: 4.18567419052124\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.04372571408748627\n",
      "\n",
      "\n",
      "Generator error: 4.155606269836426\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.045581359416246414\n",
      "\n",
      "\n",
      "Generator error: 4.246137619018555\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.26473483443260193\n",
      "\n",
      "\n",
      "Generator error: 3.531637191772461\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.09430249035358429\n",
      "\n",
      "\n",
      "Generator error: 4.189914703369141\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.0973648950457573\n",
      "\n",
      "\n",
      "Generator error: 4.341207504272461\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.045307815074920654\n",
      "\n",
      "\n",
      "Generator error: 4.907821178436279\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.037316709756851196\n",
      "\n",
      "\n",
      "Generator error: 5.155282020568848\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.03524884581565857\n",
      "\n",
      "\n",
      "Generator error: 5.646926403045654\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.02935768850147724\n",
      "\n",
      "\n",
      "Generator error: 5.555333137512207\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.04921240732073784\n",
      "\n",
      "\n",
      "Generator error: 4.778570652008057\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.06449311971664429\n",
      "\n",
      "\n",
      "Generator error: 5.2547926902771\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.10426437854766846\n",
      "\n",
      "\n",
      "Generator error: 5.010622501373291\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.04717385396361351\n",
      "\n",
      "\n",
      "Generator error: 5.199351787567139\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.01615305244922638\n",
      "\n",
      "\n",
      "Generator error: 6.023027420043945\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.0453532338142395\n",
      "\n",
      "\n",
      "Generator error: 5.139570713043213\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.022888803854584694\n",
      "\n",
      "\n",
      "Generator error: 6.074775218963623\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.022983815521001816\n",
      "\n",
      "\n",
      "Generator error: 5.55596923828125\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.019788527861237526\n",
      "\n",
      "\n",
      "Generator error: 5.691268444061279\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.047241777181625366\n",
      "\n",
      "\n",
      "Generator error: 6.5123209953308105\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.027251586318016052\n",
      "\n",
      "\n",
      "Generator error: 6.834939479827881\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.06555472314357758\n",
      "\n",
      "\n",
      "Generator error: 5.871767520904541\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.04842226207256317\n",
      "\n",
      "\n",
      "Generator error: 7.0117902755737305\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.0265765730291605\n",
      "\n",
      "\n",
      "Generator error: 6.274609565734863\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.01698356494307518\n",
      "\n",
      "\n",
      "Generator error: 6.845345973968506\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.01743272691965103\n",
      "\n",
      "\n",
      "Generator error: 6.556771755218506\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.05457206442952156\n",
      "\n",
      "\n",
      "Generator error: 5.752755641937256\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.007580717094242573\n",
      "\n",
      "\n",
      "Generator error: 6.774337291717529\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.018322406336665154\n",
      "\n",
      "\n",
      "Generator error: 6.507040023803711\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.024256382137537003\n",
      "\n",
      "\n",
      "Generator error: 6.575716972351074\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.03954987972974777\n",
      "\n",
      "\n",
      "Generator error: 6.133505821228027\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.004255414940416813\n",
      "\n",
      "\n",
      "Generator error: 7.538544178009033\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.03440919890999794\n",
      "\n",
      "\n",
      "Generator error: 6.5192365646362305\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.004335743375122547\n",
      "\n",
      "\n",
      "Generator error: 7.510033130645752\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.02362797223031521\n",
      "\n",
      "\n",
      "Generator error: 7.2608642578125\n",
      "-------\n",
      "-------\n",
      "Discriminator error: 0.008627645671367645\n",
      "\n",
      "\n",
      "Generator error: 6.899728298187256\n",
      "-------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6e457c785f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Train g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mg_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-93e5550208ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, discriminator, fake_data)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Calculate error and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Update weights with gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/DiscoDuro/Pytorch-TallerPythonGranada/env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/DiscoDuro/Pytorch-TallerPythonGranada/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "# Load data\n",
    "data = mnist_dataset()\n",
    "\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "\n",
    "# Num batches\n",
    "num_batches = len(data_loader)\n",
    "\n",
    "# total number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)\n",
    "\n",
    "generator = GeneratorNet()\n",
    "discriminator = DiscriminatorNet()\n",
    "\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(n_epochs):\n",
    "    for n_batch, (real_batch, _) in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "\n",
    "        # Train discriminator\n",
    "        real_data = Variable(images_to_vectors(real_batch))\n",
    "\n",
    "        # Generate fake data and detach \n",
    "        # (so gradients are not calculated for generator)\n",
    "        fake_data = generator(noise(N)).detach()\n",
    "\n",
    "        # Train Discriminator\n",
    "        d_error, d_pred_real, d_pred_fake = discriminator.train(real_data, fake_data)\n",
    "\n",
    "        # Train Generator\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(N))\n",
    "\n",
    "        # Train g\n",
    "        g_error = generator.train(discriminator, fake_data)\n",
    "        \n",
    "        if (n_batch) % 100 == 0:\n",
    "            print(\"-------\")\n",
    "            print(\"Discriminator error: {}\".format(d_error.data))\n",
    "            print(\"\\n\")\n",
    "            print(\"Generator error: {}\".format(g_error.data))\n",
    "            print(\"-------\")\n",
    "        \n",
    "    if (epoch) % 20 == 0:\n",
    "        print(\"Saving samples\")\n",
    "        test_images = vectors_to_images(generator(test_noise))\n",
    "        test_images = test_images.data\n",
    "        save_figures(test_images,path=\"./generated/epoch-\"+(str(epoch))+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
